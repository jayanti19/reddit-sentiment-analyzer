# -*- coding: utf-8 -*-
"""senti.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kpVKUMKN_lVZcEx7aBMcJbP82PiPvCBi

**Cell 1: Import Libraries**
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.models import load_model
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

"""**Cell 2: Load Dataset**"""

data = pd.read_csv('/content/final_data.csv')

"""** Cell 3: Preprocessing**"""

data['comment_text'] = data['comment_text'].fillna('')
comments = data['comment_text'].values
sentiments = data['sentiments'].values

label_encoder = LabelEncoder()
sentiments_encoded = label_encoder.fit_transform(sentiments)

tokenizer = Tokenizer()
tokenizer.fit_on_texts(comments)
sequences = tokenizer.texts_to_sequences(comments)

max_sequence_length = 100
word_index = tokenizer.word_index
vocab_size = len(word_index) + 1

padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')

"""# ** Cell 4: Split Data**

"""

X_train, X_test, y_train, y_test = train_test_split(padded_sequences, sentiments_encoded, test_size=0.2, random_state=42)

# One-hot encode labels
num_classes = len(np.unique(sentiments_encoded))
y_train = tf.keras.utils.to_categorical(y_train, num_classes=num_classes)
y_test = tf.keras.utils.to_categorical(y_test, num_classes=num_classes)

"""# Cell 5: Build CNN Model"""

# Cell 5: Build CNN Model
model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=128, input_length=max_sequence_length),
    Conv1D(filters=128, kernel_size=5, activation='relu'),
    GlobalMaxPooling1D(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(num_classes, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

"""# ** Cell 6: Train Model**"""

# Cell 6: Train Model
model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.1)

"""# ** Cell 7: Evaluate Model**"""

# Cell 7: Evaluate Model
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy}")

"""# Cell 8: Interactive Input and Prediction"""

# Cell 8: Interactive Input and Prediction
subreddit_filter = input("Enter the subreddit name to filter: ")
data = data[data['subreddit'] == subreddit_filter]
titles = data['title'].unique()
print("Available titles:")
for idx, title in enumerate(titles, start=1):
    print(f"{idx}. {title}")

title_choice = int(input("Choose a title by number: "))
selected_title = titles[title_choice - 1]
selected_data = data[data['title'] == selected_title]

# Prepare data for the selected title
comments = selected_data['comment_text'].fillna('').values
sequences = tokenizer.texts_to_sequences(comments)
padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')

# Predict and Analyze
predictions = model.predict(padded_sequences)
predicted_labels = np.argmax(predictions, axis=1)
predicted_sentiments = label_encoder.inverse_transform(predicted_labels)

sentiment_counts = pd.Series(predicted_sentiments).value_counts()
most_common_sentiment = sentiment_counts.idxmax()
print(f"The most common sentiment for the title '{selected_title}' is '{most_common_sentiment}'.")

"""# Save the trained model"""

# Save the trained model
model.save('sentiment_cnn_model.h5')

# Save the tokenizer
import pickle
with open('tokenizer.pkl', 'wb') as handle:
    pickle.dump(tokenizer, handle)

# Save the label encoder
with open('label_encoder.pkl', 'wb') as handle:
    pickle.dump(label_encoder, handle)

"""# Download the Saved Files"""

from google.colab import files
files.download('sentiment_cnn_model.h5')
files.download('tokenizer.pkl')
files.download('label_encoder.pkl')